{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "0428 딥러닝 (Word2Vec, 텍스트 유사도, 챗봇).ipynb",
      "provenance": [],
      "collapsed_sections": [
        "0nEFrVQ51emU",
        "ydw4u72IakFW"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GOywsE0e0QvO",
        "outputId": "dd8bf963-f353-4197-fcf9-768ed22c40c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: konlpy in /usr/local/lib/python3.7/dist-packages (0.6.0)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.2.6)\n",
            "Requirement already satisfied: JPype1>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (4.2.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install konlpy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Word2Vec\n",
        "- 구글에서 발표 (가장 많이 사용)\n",
        "1. CBOW\n",
        "- 주변 단어를 중심으로 타겟 단어 예측\n",
        "2. skip-gram\n",
        "- 타겟 단어를 중심으로 주변 단어 예측"
      ],
      "metadata": {
        "id": "0nEFrVQ51emU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import\n",
        "from gensim.models import Word2Vec\n",
        "from konlpy.tag import Komoran\n",
        "import urllib.request\n",
        "import time"
      ],
      "metadata": {
        "id": "ctDDP6b33n1H"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 리뷰 다운로드\n",
        "print(\"네이버 영화 리뷰 다운로드 시작\")\n",
        "url = \"https://raw.githubusercontent.com/e9t/nsmc/master/ratings.txt\"\n",
        "urllib.request.urlretrieve(url, filename=url.split(\"/\")[-1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TTa6Wqzf51XY",
        "outputId": "f64a8f8b-a8ab-47cc-9ced-ffdbebc1764c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "네이버 영화 리뷰 다운로드 시작\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('ratings.txt', <http.client.HTTPMessage at 0x7f5df5ea5250>)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 네이버 영화 리뷰 데이터 읽어옴\n",
        "def read_review_data(filename):\n",
        "    with open(filename, 'r') as f:\n",
        "        data = [line.split('\\t') for line in f.read().splitlines()]\n",
        "        data = data[1:] # header 제거\n",
        "    return data\n",
        "\n",
        "\n",
        "# 측정 시작\n",
        "start = time.time()\n",
        "\n",
        "# 리뷰 파일 읽어오기\n",
        "print('1) 말뭉치 데이터 읽기 시작')\n",
        "review_data = read_review_data('./ratings.txt')\n",
        "print(len(review_data)) # 리뷰 데이터 전체 개수\n",
        "print('1) 말뭉치 데이터 읽기 완료: ', time.time() - start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Au6I00tF52hl",
        "outputId": "0733375e-7f96-44bc-839a-277dd6707610"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1) 말뭉치 데이터 읽기 시작\n",
            "200000\n",
            "1) 말뭉치 데이터 읽기 완료:  0.4902045726776123\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 문장단위로 명사만 추출해 학습 입력 데이터로 만듬\n",
        "print('2) 형태소에서 명사만 추출 시작')\n",
        "komoran = Komoran()\n",
        "docs = [komoran.nouns(sentence[1]) for sentence in review_data]\n",
        "print('2) 형태소에서 명사만 추출 완료: ', time.time() - start)\n",
        "\n",
        "# word2vec 모델 학습\n",
        "print('3) word2vec 모델 학습 시작')\n",
        "model = Word2Vec(sentences=docs, size=200, window=4, min_count=2, sg=1)\n",
        "print('3) word2vec 모델 학습 완료: ', time.time() - start)\n",
        "\n",
        "# 모델 저장\n",
        "print('4) 학습된 모델 저장 시작')\n",
        "model.save('nvmc.model')\n",
        "print('4) 학습된 모델 저장 완료: ', time.time() - start)\n",
        "\n",
        "# 학습된 말뭉치 개수, 코퍼스 내 전체 단어 개수\n",
        "print(\"corpus_count : \", model.corpus_count)\n",
        "print(\"corpus_total_words : \", model.corpus_total_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tYRWkl858BPQ",
        "outputId": "6aad7fdf-bb3d-4018-f278-1a74581d60b1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2) 형태소에서 명사만 추출 시작\n",
            "2) 형태소에서 명사만 추출 완료:  289.66458535194397\n",
            "3) word2vec 모델 학습 시작\n",
            "3) word2vec 모델 학습 완료:  315.3549828529358\n",
            "4) 학습된 모델 저장 시작\n",
            "4) 학습된 모델 저장 완료:  315.7180507183075\n",
            "corpus_count :  200000\n",
            "corpus_total_words :  1076896\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "# 모델 로딩\n",
        "model = Word2Vec.load('nvmc.model')\n",
        "print(\"corpus_total_words : \", model.corpus_total_words)\n",
        "\n",
        "# '사랑'이란 단어로 생성한 단어 임베딩 벡터\n",
        "print('사랑 : ', model.wv['사랑'])\n",
        "\n",
        "# 단어 유사도 계산\n",
        "print(\"일요일 = 월요일\\t\", model.wv.similarity(w1='일요일', w2='월요일'))\n",
        "print(\"안성기 = 배우\\t\", model.wv.similarity(w1='안성기', w2='배우'))\n",
        "print(\"대기업 = 삼성\\t\", model.wv.similarity(w1='대기업', w2='삼성'))\n",
        "print(\"일요일 != 삼성\\t\", model.wv.similarity(w1='일요일', w2='삼성'))\n",
        "print(\"히어로 != 삼성\\t\", model.wv.similarity(w1='히어로', w2='삼성'))\n",
        "\n",
        "# 가장 유사한 단어 추출\n",
        "print(model.wv.most_similar(\"안성기\", topn=5))\n",
        "print(model.wv.most_similar(\"시리즈\", topn=5))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dU5L_wbu-vT-",
        "outputId": "7e2f6652-bc7b-4b39-8e6e-d68041c80688"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "corpus_total_words :  1076896\n",
            "사랑 :  [ 1.54961705e-01 -8.28421209e-03 -4.06622648e-01 -1.35005593e-01\n",
            "  2.75653362e-01 -8.24820176e-02 -5.70251532e-02  2.17864692e-01\n",
            "  5.06871620e-05  4.20566835e-02 -2.51299828e-01 -1.68055996e-01\n",
            "  3.62588197e-01  1.30160570e-01  1.11356057e-01  1.01998411e-01\n",
            " -2.39766881e-01 -1.75024360e-01 -6.94807572e-03 -8.86054859e-02\n",
            " -3.62326950e-02  2.31147602e-01 -2.75029391e-01 -9.42782778e-03\n",
            " -3.10203195e-01  2.54819751e-01  5.46838017e-03 -5.76698743e-02\n",
            "  1.15322292e-01 -3.74119550e-01  9.96087790e-02  3.65488172e-01\n",
            " -6.60605505e-02 -2.99641266e-02 -1.28484592e-01 -2.33996473e-02\n",
            "  7.60770738e-02 -1.42497882e-01 -8.54229927e-02  3.53047222e-01\n",
            "  4.05172966e-02  2.83779621e-01  5.68072796e-01  1.39622599e-01\n",
            "  2.76407272e-01 -1.65754035e-01  4.68611985e-01  2.30613112e-01\n",
            " -4.20536995e-01 -1.57247961e-01 -1.88040257e-01 -2.05474004e-01\n",
            " -3.24679077e-01  1.14672847e-01 -7.01559857e-02 -2.86520272e-01\n",
            "  2.00340584e-01  7.10725263e-02 -2.00896353e-01  3.45692128e-01\n",
            " -1.15870520e-01 -1.77457780e-01 -1.55260220e-01 -2.13598400e-01\n",
            " -1.27199039e-01  4.14673984e-01 -1.79835439e-01 -1.83562160e-01\n",
            "  3.53187114e-01  3.96854579e-02 -3.39166373e-02  3.86363678e-02\n",
            "  2.14583650e-01  1.64768517e-01 -1.20039612e-01 -1.29984975e-01\n",
            " -1.94018736e-01  2.20150694e-01  4.79380786e-02 -1.08002260e-01\n",
            " -3.85775417e-01  3.09264630e-01 -1.72255158e-01 -3.85776423e-02\n",
            " -1.12956883e-02 -4.80666041e-01 -1.25113994e-01  4.03520584e-01\n",
            " -3.68355274e-01 -1.58028081e-01  6.82653720e-03 -1.99990153e-01\n",
            " -8.60933587e-02  6.82889670e-02 -1.95836082e-01  1.51929244e-01\n",
            " -2.23075110e-03 -1.21410592e-02 -6.09799623e-01  4.46558893e-02\n",
            "  5.21156967e-01 -1.45137742e-01 -1.88152380e-02 -7.24804997e-02\n",
            "  1.82914317e-01 -1.65518537e-01  1.92205250e-01  3.96264523e-01\n",
            " -4.27823663e-02  2.47323483e-01  2.91913599e-02  6.28012419e-02\n",
            "  1.85748816e-01 -4.00308162e-01 -8.32327362e-03 -3.40268672e-01\n",
            "  1.59490302e-01  3.39347571e-01 -2.10128263e-01 -4.19425368e-02\n",
            " -3.98904411e-03  2.39576474e-01  6.80530295e-02  3.55532467e-01\n",
            "  4.32354305e-03  1.27627552e-01 -4.15397376e-01 -6.44889921e-02\n",
            "  3.47677708e-01  2.57644922e-01  1.45379946e-01  1.18561737e-01\n",
            "  1.01659279e-02 -9.04048532e-02  7.17824101e-02 -2.01552972e-01\n",
            " -6.41237423e-02 -2.75315732e-01 -3.75780076e-01 -4.00573879e-01\n",
            " -4.80884016e-02  4.59273010e-02  1.41727701e-01  1.64032504e-01\n",
            "  5.64525306e-01  1.22535869e-01 -3.20707202e-01  5.56508303e-02\n",
            " -1.36034582e-02  5.95836677e-02  2.40314424e-01  3.82506102e-01\n",
            " -3.38821441e-01 -3.56307536e-01 -1.38631552e-01 -5.85452393e-02\n",
            " -3.59485507e-01  1.35335490e-01  1.32610172e-01  4.85460997e-01\n",
            "  5.13090529e-02 -4.00763631e-01  4.73514229e-01 -1.05378285e-01\n",
            "  2.70306051e-01  3.86735380e-01  5.84654287e-02 -4.68790121e-02\n",
            " -5.30931838e-02  1.50182739e-01  9.54748690e-02 -6.60770595e-01\n",
            "  3.90696526e-01  1.68652371e-01  7.86404014e-02 -1.31337374e-01\n",
            " -5.18181205e-01  2.84989119e-01 -2.73379862e-01  3.32470506e-01\n",
            "  1.47130266e-01  2.13645697e-01  1.17289476e-01 -2.19558135e-01\n",
            " -2.24749520e-01 -4.72326159e-01  1.91113412e-01  2.61508703e-01\n",
            " -6.12627529e-03 -3.10006887e-01 -5.97339392e-01  3.59163471e-02\n",
            "  1.03126220e-01  6.42039105e-02  4.04993445e-01 -3.54844444e-02\n",
            "  1.06028887e-02  3.43096197e-01  7.96872377e-02 -3.08376729e-01]\n",
            "일요일 = 월요일\t 0.9187525\n",
            "안성기 = 배우\t 0.72217363\n",
            "대기업 = 삼성\t 0.8878077\n",
            "일요일 != 삼성\t 0.6543765\n",
            "히어로 != 삼성\t 0.46045095\n",
            "[('박신양', 0.9590926170349121), ('한석규', 0.9509379267692566), ('황정민', 0.9468600749969482), ('최강희', 0.9465543627738953), ('김하늘', 0.9459171891212463)]\n",
            "[('포터', 0.8331488370895386), ('반지의 제왕', 0.8139418363571167), ('엑스맨', 0.8135151863098145), ('해리', 0.8069702386856079), ('스타워즈', 0.7964768409729004)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 텍스트 유사도\n",
        "\n"
      ],
      "metadata": {
        "id": "XT82DvbvXWzU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## n-gram 유사도\n",
        "- 이웃 단어의 출현 빈도를 통계적으로 표현"
      ],
      "metadata": {
        "id": "ydw4u72IakFW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from konlpy.tag import Komoran"
      ],
      "metadata": {
        "id": "Imgkd9jbYspd"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 어절 단위 n-gram\n",
        "def word_ngram(bow, num_gram):\n",
        "    text = tuple(bow)\n",
        "    ngrams = [text[x:x + num_gram] for x in range(0, len(text))]\n",
        "    return tuple(ngrams)"
      ],
      "metadata": {
        "id": "a7HKCytjXroW"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 음절 n-gram 분석\n",
        "def phoneme_ngram(bow, num_gram):\n",
        "    sentence = ' '.join(bow)\n",
        "    text = tuple(sentence)\n",
        "    slen = len(text)\n",
        "    ngrams = [text[x:x + num_gram] for x in range(0, slen)]\n",
        "    return ngrams"
      ],
      "metadata": {
        "id": "jIn2nidNX7w1"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 유사도 계산\n",
        "def similarity(doc1, doc2):\n",
        "    cnt = 0\n",
        "    for token in doc1:\n",
        "        if token in doc2:\n",
        "            cnt = cnt + 1\n",
        "\n",
        "    return cnt/len(doc1)"
      ],
      "metadata": {
        "id": "-p2KVRdZaXkG"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2-gram 유사도\n",
        "sentence1 = '6월에 뉴턴은 선생님의 제안으로 트리니티에 입학하였다'\n",
        "sentence2 = '6월에 뉴턴은 선생님의 제안으로 대학교에 입학하였다'\n",
        "sentence3 = '나는 맛잇는 밥을 뉴턴 선생님과 함께 먹었습니다.'\n",
        "\n",
        "komoran = Komoran()\n",
        "bow1 = komoran.nouns(sentence1)\n",
        "bow2 = komoran.nouns(sentence2)\n",
        "bow3 = komoran.nouns(sentence3)\n",
        "\n",
        "doc1 = word_ngram(bow1, 2)\n",
        "doc2 = word_ngram(bow2, 2)\n",
        "doc3 = word_ngram(bow3, 2)\n",
        "\n",
        "print(\"doc1 :\",doc1)\n",
        "print(\"doc2 :\",doc2)\n",
        "print(\"doc3 :\",doc3)\n",
        "\n",
        "r1 = similarity(doc1, doc2)\n",
        "r2 = similarity(doc3, doc1)\n",
        "print(r1)\n",
        "print(r2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NvbXIu3MYPK3",
        "outputId": "1a3ee669-b562-4cc1-c464-ad938bdc61a6"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "doc1 : (('6월', '뉴턴'), ('뉴턴', '선생님'), ('선생님', '제안'), ('제안', '트리니티'), ('트리니티', '입학'), ('입학',))\n",
            "doc2 : (('6월', '뉴턴'), ('뉴턴', '선생님'), ('선생님', '제안'), ('제안', '대학교'), ('대학교', '입학'), ('입학',))\n",
            "doc3 : (('맛', '밥'), ('밥', '뉴턴'), ('뉴턴', '선생'), ('선생', '님과 함께'), ('님과 함께',))\n",
            "0.6666666666666666\n",
            "0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 코사인 유사도\n",
        "- 벡터의 크기가 중요하지 않을때 사용\n",
        "- 다양한 차원에서 적용가능. 실무에서 많이 사용"
      ],
      "metadata": {
        "id": "8KjJ4ZpWafbQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from konlpy.tag import Komoran\n",
        "import numpy as np\n",
        "from numpy import dot\n",
        "from numpy.linalg import norm"
      ],
      "metadata": {
        "id": "G3Umeq59agcT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 코사인 유사도 계산\n",
        "def cos_sim(vec1, vec2):\n",
        "    return dot(vec1, vec2) / (norm(vec1) * norm(vec2))"
      ],
      "metadata": {
        "id": "s6e3_PsVa061"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TDM 만들기\n",
        "def make_term_doc_mat(sentence_bow, word_dics):\n",
        "    freq_mat = {}\n",
        "\n",
        "    for word in word_dics:\n",
        "        freq_mat[word] = 0\n",
        "\n",
        "    for word in word_dics:\n",
        "        if word in sentence_bow:\n",
        "            freq_mat[word] += 1\n",
        "\n",
        "    return freq_mat"
      ],
      "metadata": {
        "id": "CdlcWpNka2YX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 회귀\n",
        "Dense(1) ->마지막  \n",
        "model.compile(optimizer = 'adam', loss='mse')\n",
        "\n",
        "# 이항분류 (0,1)\n",
        "Dense(1, activation = 'sigmoid') -> 마지막  \n",
        "model.compile(optimizeer = 'adam', loss='binary_crossentropy', metrics=['acc'])\n",
        "\n",
        "# 다중분류\n",
        "Dense(10, activation = 'softmax') -> 마지막 (10 : 분류항 개수)  \n",
        "model.compile(optimizer = 'adam', loss='sparse_categorical_crossentropy', metrics=['acc'])\n",
        "\n",
        "# 다중분류 (희소벡터 = 원핫인코딩 반환)\n",
        "model.compile(optimizer = 'adam', loss='categorical_crossentropy', metrics=['acc'])"
      ],
      "metadata": {
        "id": "roPzi46CeP9R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 합성곱 (convolution)\n",
        "- 특징을 추출\n",
        "- 원하는 패턴이 얼마나 있나 계산\n",
        "\n",
        "# 풀링\n",
        "- 메모리의 크기를 줄이기 위해 사용하는것 => 풀링\n",
        "- 커널을 안줄어들게 하기위해 패딩 사용\n"
      ],
      "metadata": {
        "id": "AO2TbUhKgixu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 챗봇 문답 감정 분류 모델"
      ],
      "metadata": {
        "id": "EV80zs3RpyGS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow"
      ],
      "metadata": {
        "id": "qvntR98Nqfna"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 필요한 모듈 임포트\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import preprocessing\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Embedding, Dense, Dropout, Conv1D, GlobalMaxPool1D, concatenate"
      ],
      "metadata": {
        "id": "j51bhw2sd-Kw"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 읽어오기\n",
        "url = \"https://raw.githubusercontent.com/songys/Chatbot_data/master/ChatbotData.csv\"\n",
        "data = pd.read_csv(url, delimiter=',')\n",
        "features = data['Q'].tolist()\n",
        "labels = data['label'].tolist()\n",
        "\n",
        "# 단어 인덱스 시퀀스 벡터\n",
        "corpus = [preprocessing.text.text_to_word_sequence(text) for text in features]"
      ],
      "metadata": {
        "id": "KwmpVoihqH6o"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = preprocessing.text.Tokenizer()\n",
        "tokenizer.fit_on_texts(corpus)\n",
        "sequences = tokenizer.texts_to_sequences(corpus)\n",
        "word_index = tokenizer.word_index\n",
        "MAX_SEQ_LEN = 15  # 단어 시퀀스 벡터 크기\n",
        "padded_seqs = preprocessing.sequence.pad_sequences(sequences, maxlen=MAX_SEQ_LEN, padding='post')"
      ],
      "metadata": {
        "id": "pL52Y387qo_J"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습용, 검증용, 테스트용 데이터셋 생성 ➌\n",
        "# 학습셋:검증셋:테스트셋 = 7:2:1\n",
        "ds = tf.data.Dataset.from_tensor_slices((padded_seqs, labels))\n",
        "ds = ds.shuffle(len(features))\n",
        "train_size = int(len(padded_seqs) * 0.7)\n",
        "val_size = int(len(padded_seqs) * 0.2)\n",
        "test_size = int(len(padded_seqs) * 0.1)\n",
        "train_ds = ds.take(train_size).batch(20)\n",
        "val_ds = ds.skip(train_size).take(val_size).batch(20)\n",
        "test_ds = ds.skip(train_size + val_size).take(test_size).batch(20)"
      ],
      "metadata": {
        "id": "yz-C54cosBXG"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 하이퍼파라미터 설정\n",
        "# 함수형 코드\n",
        "dropout_prob = 0.5\n",
        "EMB_SIZE = 128\n",
        "EPOCH = 5\n",
        "VOCAB_SIZE = len(word_index) + 1  # 전체 단어 수\n",
        "\n",
        "# CNN 모델 정의\n",
        "input_layer = Input(shape=(MAX_SEQ_LEN,))\n",
        "embedding_layer = Embedding(VOCAB_SIZE, EMB_SIZE, input_length=MAX_SEQ_LEN)(input_layer)\n",
        "dropout_emb = Dropout(rate=dropout_prob)(embedding_layer)\n",
        "\n",
        "conv1 = Conv1D(filters=128, kernel_size=3, padding='valid', activation=tf.nn.relu)(dropout_emb)\n",
        "pool1 = GlobalMaxPool1D()(conv1)\n",
        "conv2 = Conv1D(filters=128, kernel_size=4, padding='valid', activation=tf.nn.relu)(dropout_emb)\n",
        "pool2 = GlobalMaxPool1D()(conv2)\n",
        "conv3 = Conv1D(filters=128, kernel_size=5, padding='valid', activation=tf.nn.relu)(dropout_emb)\n",
        "pool3 = GlobalMaxPool1D()(conv3)"
      ],
      "metadata": {
        "id": "U6T9qHCLtI9J"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3, 4, 5- gram 이후 합치기\n",
        "concat = concatenate([pool1, pool2, pool3])\n",
        "hidden = Dense(128, activation=tf.nn.relu)(concat)\n",
        "dropout_hidden = Dropout(rate=dropout_prob)(hidden)\n",
        "logits = Dense(3, name='logits')(dropout_hidden)\n",
        "predictions = Dense(3, activation=tf.nn.softmax)(logits)\n",
        "\n",
        "# 모델 생성\n",
        "model = Model(inputs=input_layer, outputs=predictions)\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# 모델 학습\n",
        "model.fit(train_ds, validation_data=val_ds, epochs=EPOCH, verbose=1)\n",
        "\n",
        "# 모델 평가(테스트 데이터셋 이용)\n",
        "loss, accuracy = model.evaluate(test_ds, verbose=1)\n",
        "print('Accuracy: %f' % (accuracy * 100))\n",
        "print('loss: %f' % (loss))\n",
        "\n",
        "# 모델 저장\n",
        "model.save('cnn_model.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kmJavu1dtm_p",
        "outputId": "ec842e5a-df9e-476c-dc28-a51dcc44cae2"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "414/414 [==============================] - 18s 13ms/step - loss: 0.9035 - accuracy: 0.5644 - val_loss: 0.6793 - val_accuracy: 0.7039\n",
            "Epoch 2/5\n",
            "414/414 [==============================] - 4s 11ms/step - loss: 0.5733 - accuracy: 0.7815 - val_loss: 0.2920 - val_accuracy: 0.9006\n",
            "Epoch 3/5\n",
            "414/414 [==============================] - 4s 11ms/step - loss: 0.3305 - accuracy: 0.8881 - val_loss: 0.1699 - val_accuracy: 0.9425\n",
            "Epoch 4/5\n",
            "414/414 [==============================] - 4s 11ms/step - loss: 0.2135 - accuracy: 0.9328 - val_loss: 0.0992 - val_accuracy: 0.9657\n",
            "Epoch 5/5\n",
            "414/414 [==============================] - 5s 11ms/step - loss: 0.1369 - accuracy: 0.9577 - val_loss: 0.0739 - val_accuracy: 0.9759\n",
            "60/60 [==============================] - 0s 5ms/step - loss: 0.0637 - accuracy: 0.9797\n",
            "Accuracy: 97.969544\n",
            "loss: 0.063654\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras import preprocessing\n",
        "\n",
        "# 데이터 읽어오기\n",
        "url = \"https://raw.githubusercontent.com/songys/Chatbot_data/master/ChatbotData.csv\"\n",
        "data = pd.read_csv(url, delimiter=',')\n",
        "features = data['Q'].tolist()\n",
        "labels = data['label'].tolist()\n",
        "\n",
        "# 단어 인덱스 시퀀스 벡터\n",
        "corpus = [preprocessing.text.text_to_word_sequence(text) for text in features]\n",
        "tokenizer = preprocessing.text.Tokenizer()\n",
        "tokenizer.fit_on_texts(corpus)\n",
        "sequences = tokenizer.texts_to_sequences(corpus)\n",
        "MAX_SEQ_LEN = 15 # 단어 시퀀스 벡터 크기\n",
        "padded_seqs = preprocessing.sequence.pad_sequences(sequences, maxlen=MAX_SEQ_LEN, padding='post')"
      ],
      "metadata": {
        "id": "6vv7b6ZczDz7"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 테스트용 데이터셋 생성\n",
        "ds = tf.data.Dataset.from_tensor_slices((padded_seqs, labels))\n",
        "ds = ds.shuffle(len(features))\n",
        "test_ds = ds.take(2000).batch(20) # 테스트 데이터셋\n",
        "\n",
        "# 감정 분류 CNN 모델 불러오기\n",
        "model = load_model('cnn_model.h5')\n",
        "model.summary()\n",
        "model.evaluate(test_ds, verbose=2)\n",
        "\n",
        "# 테스트용 데이터셋의 10212번째 데이터 출력\n",
        "print(\"단어 시퀀스 : \", corpus[10212])\n",
        "print(\"단어 인덱스 시퀀스 : \", padded_seqs[10212])\n",
        "print(\"문장 분류(정답) : \", labels[10212])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oiSH70ZT0FZ7",
        "outputId": "a650d86e-8bfa-4f20-c049-d474fe194d4e"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_2 (InputLayer)           [(None, 15)]         0           []                               \n",
            "                                                                                                  \n",
            " embedding_1 (Embedding)        (None, 15, 128)      1715072     ['input_2[0][0]']                \n",
            "                                                                                                  \n",
            " dropout_1 (Dropout)            (None, 15, 128)      0           ['embedding_1[0][0]']            \n",
            "                                                                                                  \n",
            " conv1d_3 (Conv1D)              (None, 13, 128)      49280       ['dropout_1[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_4 (Conv1D)              (None, 12, 128)      65664       ['dropout_1[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_5 (Conv1D)              (None, 11, 128)      82048       ['dropout_1[0][0]']              \n",
            "                                                                                                  \n",
            " global_max_pooling1d_3 (Global  (None, 128)         0           ['conv1d_3[0][0]']               \n",
            " MaxPooling1D)                                                                                    \n",
            "                                                                                                  \n",
            " global_max_pooling1d_4 (Global  (None, 128)         0           ['conv1d_4[0][0]']               \n",
            " MaxPooling1D)                                                                                    \n",
            "                                                                                                  \n",
            " global_max_pooling1d_5 (Global  (None, 128)         0           ['conv1d_5[0][0]']               \n",
            " MaxPooling1D)                                                                                    \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)      (None, 384)          0           ['global_max_pooling1d_3[0][0]', \n",
            "                                                                  'global_max_pooling1d_4[0][0]', \n",
            "                                                                  'global_max_pooling1d_5[0][0]'] \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 128)          49280       ['concatenate[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_2 (Dropout)            (None, 128)          0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " logits (Dense)                 (None, 3)            387         ['dropout_2[0][0]']              \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 3)            12          ['logits[0][0]']                 \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 1,961,743\n",
            "Trainable params: 1,961,743\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "100/100 - 1s - loss: 0.0902 - accuracy: 0.9705 - 706ms/epoch - 7ms/step\n",
            "단어 시퀀스 :  ['썸', '타는', '여자가', '남사친', '만나러', '간다는데', '뭐라', '해']\n",
            "단어 인덱스 시퀀스 :  [   13    61   127  4320  1333 12162   856    31     0     0     0     0\n",
            "     0     0     0]\n",
            "문장 분류(정답) :  2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 테스트용 데이터셋의 10212번째 데이터 감정 예측\n",
        "picks = [10212]\n",
        "predict = model.predict(padded_seqs[picks])\n",
        "predict_class = tf.math.argmax(predict, axis=1)\n",
        "print(\"감정 예측 점수 : \", predict)\n",
        "print(\"감정 예측 클래스 : \", predict_class.numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dQxCKVhp0Jrt",
        "outputId": "33c797a2-93a3-411f-c286-3ebc9599adf0"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "감정 예측 점수 :  [[1.2372795e-05 1.5071144e-05 9.9997258e-01]]\n",
            "감정 예측 클래스 :  [2]\n"
          ]
        }
      ]
    }
  ]
}